{% from 'macro_term_table.jinja2' import table_classes %}
{% from 'macro_term_table.jinja2' import table_properties %}
{% from 'macro_term_table.jinja2' import list_hierarchy, index_concepts, anchor %}
{% from 'macro_dpv_document_family.jinja2' import dpv_document_family, sotd, funding_acknowledgements, contributors_list, authors_list %}
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>{% block title %}{{data[vocab_name+'-metadata']['dct:title']}}{% endblock title %}</title>
  <script src="https://www.w3.org/Tools/respec/respec-w3c" class="remove" defer></script>
  {% block RESPEC %}
  <script class="remove">
   // All config options at https://respec.org/docs/ 
   var respecConfig = {
    shortName: "eu-aiact",
    title: "{{data[vocab_name+'-metadata']['dct:title']}} ({{vocab_name|upper}})",
    subtitle: "version {{data[vocab_name+'-metadata']['schema:version']}}",
    publishDate: "{{data[vocab_name+'-metadata']['dct:modified']}}",
    specStatus: "{{vocab_name|get_document_status}}",
    group: "dpvcg",
    latestVersion: "https://w3id.org/dpv/legal/eu/aiact",
    canonicalUri: "https://w3id.org/dpv/legal/eu/aiact",
    edDraftURI: "https://dev.dpvcg.org/legal/eu/aiact",
    github: "w3c/dpv",
    subjectPrefix: "[eu-aiact]",
    doJsonLd: true,
    lint: { "no-unused-dfns": false, "local-refs-exist": true },
    editors: [
    {
        "name": "Delaram Golpayegani",
        "company": "{{ "Delaram Golpayegani" | generate_author_affiliation }}"
    },
    {
      name: "Harshvardhan J. Pandit",
      url: "https://harshp.com",
      "company": "{{ "Harshvardhan J. Pandit" | generate_author_affiliation }}"
    }
    ],
    authors: {{ authors_list(data, vocab_name) }},
    otherLinks: [
      {
        "key": "This Release",
        "data": [
            {
              "value": "https://w3id.org/dpv/{{DPV_VERSION}}/legal/eu/aiact",
              "href": "https://w3id.org/dpv/{{DPV_VERSION}}/legal/eu/aiact"
            }
        ]
      },
      {
        "key": "Previous Release",
        "data": [
            {
              "value": "https://w3id.org/dpv/{{DPV_PREVIOUS_VERSION}}/legal/eu/aiact",
              "href": "https://w3id.org/dpv/{{DPV_PREVIOUS_VERSION}}/legal/eu/aiact"
            }
        ]
      },
      {
        "key": "Changelog",
        "data": [
            {
              "value": "Changelog for v{{DPV_VERSION}}",
              "href": "#changelog"
            }
        ]
      },
      {
        "key": "Key Publications",
        "data": [
            {
              "value": "Data Privacy Vocabulary (DPV) -- Version 2.0 (2024)",
              "href": "https://doi.org/10.1007/978-3-031-77847-6_10"
            },
            {
              "value": "To Be High-Risk, or Not To Be—Semantic Specifications and Implications of the AI Act’s High-Risk AI Applications and Harmonised Standards (2023)",
              "href": "https://doi.org/10.1145/3593013.3594050"
            },
            {
              "value": "AIRO: an Ontology for Representing AI Risks based on the Proposed EU AI Act and ISO Risk Management Standards (2022)",
              "href": "https://doi.org/10.3233/SSW220008"
            }
        ]
      }
    ],
    localBiblio: {%  include 'references.json' %}
  };
</script>
{% endblock RESPEC %}
<link rel="stylesheet" type="text/css" href="../../../diagrams/common.css">
<link rel="shortcut icon" href="../../../diagrams/favicon-16x16.png" type="image/x-icon" sizes="16x16" />
  <link rel="shortcut icon" href="../../../diagrams/favicon-32x32.png" type="image/x-icon" sizes="32x32" />
</head>
<body>
  {{ contributors_list(data, vocab_name) }}
{% block ABSTRACT %}
  <section id="abstract">
    <p>The EU-AIAct extension extends the [[[DPV]]] to provide concepts such as systems, purposes, risks, roles, documentation, and assessments based on the [[[AIAct]]]. The canonical URL for EU-AIAct extension is <a href="https://w3id.org/dpv/legal/eu/aiact">https://w3id.org/dpv/legal/eu/aiact</a>, the namespace for terms is <a href="https://w3id.org/dpv/legal/eu/aiact#"><code>https://w3id.org/dpv/legal/eu/aiact#</code></a>, the suggested prefix is <code>eu-aiact</code>, and this document along with source and releases are available at <a href="https://github.com/w3c/dpv">https://github.com/w3c/dpv</a>.</p>
  </section>
    {{ sotd(DPV_VERSION=DPV_VERSION, metadata=data[vocab_name+'-metadata']) }}
    {{ dpv_document_family(document='eu-aiact-spec') }}
{% endblock ABSTRACT %}
{% block INTRODUCTION %}
  <section id="motivation">
    <h2>Introduction</h2>
    <p>The [[[AIAct]]] is a landmark regulation which regulates the use of Artificial Intelligence (AI) technologies. To support its implementation, the [[EU-AIAct]] extension provides relevant concepts which can be used to describe the facts of how AI is being developed and used within use-cases, and to produce the required documentation necessary for establishing or verifying compliance with the [[AIAct]]. Concepts in this extension extend relevant concepts defined in [[[DPV]]], as well as those defined in [[[AI]]] and [[[TECH]]] extensions.</p>
    <p>At the moment, the [[EU-AIAct]] extension provides the following concepts and taxonomies as defined or interpreted from the [[AIAct]]:</p>
    <ul>
      <li><a href="#vocab-system">AI systems</a>, such as general purpose AI systems or those using biometrics</li>
      <li><a href="#vocab-capability">Capabilities</a>, such as to produce deep fakes or perform biometric identification</li>
      <li><a href="#vocab-risk">Risk categorisations</a>, such as defining serious incidents or systemic risks at EU level</li>
      <li><a href="#vocab-data">Data categorisations</a>, such as training and testing data</li>
      <li><a href="#vocab-roles">Legal roles</a>, such as the AI Deployers and Providers</li>
      <li><a href="#vocab-docs">Documentation</a>, such as technical documentation </li>
      <li><a href="#vocab-status">Statuses</a>, such as market availability and supply of services</li>
      <li><a href="#vocab-assessment">Assessments</a>, such as conformity assessment and fundamental rights impact assessment</li>
      <li><a href="#vocab-misc">other concepts</a>, such as substantial modification and reasonably foreseeable misuses</li>
    </ul>
  </section>
{% endblock INTRODUCTION %}
{% block VOCAB %}

<section id="mapping">
  <h2>Mapping AI Act to DPV</h2>
  <table style="font-size: 0.8rem;">
    <tr>
        <td>Article 3</td>
        <td>Definitions</td>
        <td>Covered fully</td>
    </tr>
    <tr>
        <td>Article 4</td>
        <td>AI Literacy</td>
        <td>[=AILiteracy=] with [[PD]] extension for describing audience</td>
    </tr>
    <tr>
        <td>Article 5</td>
        <td>Prohibited AI practices</td>
        <td><a href="vocab-prohibited-systems">Prohibited AI Systems</a></td>
    </tr>
    <tr>
        <td>Article 6</td>
        <td>High-Risk AI systems</td>
        <td><a href="vocab-highrisk-systems">High-Risk AI Systems</a></td>
    </tr>
    <tr>
        <td>Article 7</td>
        <td>Amendmends to Annex III</td>
        <td><a href="AI-incidents">Future Work</a></td>
    </tr>
    <tr>
        <td>Article 9</td>
        <td>Risk Management System</td>
        <td>[[DPV]], [[RISK]], [[AI]], [[EU-AIAct]]</td>
    </tr>
    <tr>
        <td>Aricle 10</td>
        <td>Data Governance</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 11</td>
        <td>Technical Documentation</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 12</td>
        <td>Record Keeping</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 13</td>
        <td>Transparency to Deployers</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 14</td>
        <td>Human Oversight</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 15</td>
        <td>Accuracy, Robustness, Cybersecurity</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 16</td>
        <td>Obligations of providers of High-Risk AI Systems</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 17</td>
        <td>Quality Management System</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 18</td>
        <td>Documentation Keeping</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 19</td>
        <td>Automatically generated logs</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 20</td>
        <td>Corrective actions and duty of information</td>
        <td>Future Work</td>
    </tr>
    <tr>
        <td>Article 22</td>
        <td>Authorised Representatives</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 23</td>
        <td>Obligations of importers</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 24</td>
        <td>Obligations of distributors</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 25</td>
        <td>Responsibilities along the AI value chain</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 26</td>
        <td>Obligations of deployers of high-risk AI systems</td>
        <td></td>
    </tr>
    <tr>
        <td>Article 27</td>
        <td>Fundemantal Rights Impact Assessment (FRIA)</td>
        <td></td>
    </tr>
    <tr>
        <td>Section 4</td>
        <td>Notifying Authorities</td>
        <td></td>
    </tr>
    <tr>
        <td>Section 5</td>
        <td>Standards, conformity assessment, certificates, registration</td>
        <td></td>
    </tr>
</table>
</section>

<section id="vocab">
  <h2>Concepts for AI Act</h2>

  <section id="vocab-system">
    <h3>AI System</h3>
    <p>The concept [=AISystem=] represents AI systems as defined in the AI Act.</p>
    
    {{ list_hierarchy(modules['system']['classes']) }}
  </section>

  <section id="vocab-capability">
    <h3>Purposes &amp; Capabilities</h3>
    <p>These concepts represent the purposes and capabilities defined within the AI Act. The use of 'capability' here refers to the capability of the technology to produce, perform, or achieve something, which is expressed in the [[TECH]] extension as <code>tech:Capability</code> and extended in the [[AI]] extension as <code>ai:Capability</code>.</p>
    
    {{ list_hierarchy(modules['capability']['classes']) }}
  </section>

  <section id="vocab-risk">
    <h3>Risk</h3>
    <p>The AI Act has a specific definition of risk as "combination of the probability of an occurrence of harm and the severity of that harm", which is represented using the concept [=Risk=] within this extension. This definition is a more specific form of {{anchor('dpv:Risk')}}, and hence the existing relations can be used to describe its severity and likelihood.</p>
    {{ list_hierarchy(modules['risk']['classes']) }}
  </section>

  <section id="vocab-risk-levels">
    <h2>Risk Levels</h2>
    <p>The interpretation of the AI Act uses terminology referring to <i>Risk Levels</i> which determine the level of obligations under the Act. To represent these, the concept [=RiskLevel=] is defined and instantiated to represent the commonly used concepts for representing prohibited, high-risk, transparency required, and minimal risk levels. The taxonomy also contains two additional concepts to represent not-prohibited and not-high-risk to support documenting outcomes of assessments that test prohibited and high-risk categorisations respectively. To associate the EU AI Act specific risk level, the property [=hasRiskLevel=] has been extended from {{anchor('dpv:hasRiskLevel')}}.</p>
    {{ list_hierarchy(modules['risk-levels']['classes'], head='eu-aiact:RiskLevel') }}
    <aside class="example" title="Representing the Risk Level as per AI Act">
      <p>The example shows how the risk level for the AI Act can be associated with a process or with a specific AI system.</p>
<pre><code># method 1: Process/Services
ex:SomeService a dpv:Process ;
  dpv:hasPurpose ex:SomePurpose ; # from DPV purpose taxonomy
  ai:hasTechnique ex:SomeTechnique ; # from AI technique taxonomy
  dpv:hasRiskLevel eu-aiact:RiskLevelHighAnnexIII .

ex:SomeSystem a eu-aiact:AISystem ;
  dpv:hasRiskLevel eu-aiact:RiskLevelHighAnnexIII .
ex:SomeProcess a dpv:Process ;
  ai:hasAISystem ex:SomeSystem . # uses high-risk AI
</code></pre>
    </aside>
    <aside class="note" title="Scope vs Risk Level">
      <p>The concept of "Risk Level" should not be mixed with the "scope" of the AI Act. The concept of [=RiskLevelMinimal=] is used to represent AI systems or practices that fit the definition of AI and AI systems under the Act but are not prohibited, high-risk, or require transparency. This is distinct from AI systems and practices which do not fit the (rigid and specific) definitions of AI and AI systems under the AI Act. This reflects the procedure where you first determine whether a system is an AI system as defined by the AI Act (scope determination) and then how it is categorised under the AI Act (risk level determination).</p>
    </aside>
  </section>

  <section id="vocab-data">
    <h3>Data</h3>
    <p>The AI Act defines different categories of data as used to develop and deploy AI systems, such as [=TrainingData=] and [=TestingData=]. It also uses [=BiometricData=] which is a special category of personal data under [[GDPR]].</p>
    
    {{ list_hierarchy(modules['data']['classes']) }}
  </section>

  <section id="vocab-roles">
    <h3>Entity Roles</h3>
    <p>AI Roles describe the categorisation of entities based on the role they take in developing, providing, deploying, or using an AI system as defined within the [[AIAct]].</p> 
    
    {{ list_hierarchy(modules['roles']['classes']) }}
  </section>

  <section id="vocab-docs">
    <h3>Documentation</h3>
    <p>The documentation associated with AI, AI systems, and other processes defined within the AI Act. These are described using <code>tech:Documentation</code> from the [[TECH]] extension.</p>

    <aside class="note" title="Future Work: We welcome discussions and contributions regarding representing AI Act's Technical Documentation in a machine-readable manner using the DPV vocabularies.">
    </aside>
    
    {{ list_hierarchy(modules['docs']['classes']) }}
  </section>

  <section id="vocab-status">
    <h3>Statuses</h3>
    <p>Different statuses are described or implied within the [[AIAct]], such as [=MarketAvailabilityStatus=] regarding whether the AI system is available on the market, and [=ServiceSupplyStatus=] regarding whether the AI system has been supplied. These statuses reflect the requirement to describe the state of the AI system and its use, which has implications in terms of requirements and obligations.</p>
    
    {{ list_hierarchy(modules['status']['classes']) }}
  </section>

  <section id="vocab-assessment">
    <h3>Assessments</h3>
    <p>Compliance with requirements of the [[AIAct]], e.g affixing [=CEMarking=] or implementing a [=PostMarketMonitoringSystem=], requires various types of assessments. These are described using [[DPV]] concepts such as <code>dpv:CertificationSeal</code> and <code>dpv:ImpactAssessment</code>.</p>
    
    {{ list_hierarchy(modules['assessment']['classes']) }}
  </section>

  <section id="vocab-compliance">
    <h3>Compliance</h3>
    <p>The concepts in this section reflect the status of processing operations being in <i>compliance with AI Act</i>, by extending the <code>ComplianceStatus</code> from DPV for AI Act. It does not define the requirements for compliance itself. To indicate these, the relation <code>dpv:hasLawfulness</code> can be used.</p>
    <aside class="example" title="Indicating status of AI Act lawfulness">
      <p>This example shows the use of EU-AIAct to indicate the state of AIAct lawfulness associated with two processes. The first is asserted to be compliant, while the compliance status for the second is unknown at the moment. Both processes assert the applicability of AI Act for brevity.</p>
      <pre class="nohighlight"><code>ex:PDH1 a dpv:Process ;
  dpv:hasApplicableLaw legal-eu:AIAct ;
  dpv:hasLawfulness eu-aiact:AIActCompliant .

ex:PDH3 a dpv:Process ;
  dpv:hasApplicableLaw legal-eu:AIAct ;
  dpv:hasLawfulness eu-aiact:AIActComplianceUnknown .
      </code></pre>
    </aside>
    {{ list_hierarchy(modules['compliance']['classes']) }}
  </section>

  <section id="vocab-sector">
    <h3>Sectors</h3>
    <p>The [[AIAct]] implicitly refers to various sectors, such as when stating the high-risk uses of AI systems in Annex III. To enable referring to such implied sectors, this extension provides an ad-hoc taxonomy. This taxonomy may change in the future as sectors are also an important concept in other laws, such as [[EU-NIS2]]</p>
    
    {{ list_hierarchy(modules['sector']['classes']) }}
  </section>

  <section id="vocab-misc">
    <h3>Misc. Concepts</h3>
    <p>These concepts currently do not fit within the other stated categories, and are pooled together under a 'misc.' label while the vocabulary is being further developed.</p>
    
    {{ list_hierarchy(modules['misc']['classes']) }}
  </section>
</section>

<section id="high-risk-prohibited">
  <h2>High-Risk &amp; Prohibited AI Systems</h2>

  <section id="vocab-highrisk-systems">
    <h3>High-Risk AI Systems</h3>
    <aside class="note" title="Work in progress">
      <p>This section reflects the work in progress for expressing specific applications described in Annex III using DPV concepts, based on the work "<a href="https://doi.org/10.1145/3593013.3594050">To Be High-Risk, or Not To Be—Semantic Specifications and Implications of the AI Act’s High-Risk AI Applications and Harmonised Standards (Golpayegani et. al - FAccT 2023)</a>". We welcome help/support for this.</p>
    </aside>
    <p>The AI Act describes high-risk AI systems in Article 6, with two categories: Article 6(1) which refers to Annex I containing sectorial laws, and Article 6(2) referring to the specific AI applications in Annex III. The risk levels section earlier in the document described the corresponding risk levels that reflect this structure. To represent [=AISystem=] that are thus categorised as high-risk, [=HighRiskAISystem=] is provided. To specifically refer to AI Systems that are high-risk due to sectorial laws Annex I, the concept [=HighRiskAISystem-A6-1=] is defined. Similarly, to represent AI systems that are high-risk due to applications in Annex III, the concept [=HighRiskAISystem-A6-2=] is defined.</p>
    <p>Both Annex I defines several high-risk systems. These are represented as instances of [=HighRiskAISystem=] and are more specific forms of [=HighRiskAISystem-A6-1=] (i.e. using `skos:broader`). Similarly, Annex III contains several applications, which are represented as more specific forms of [=HighRiskAISystem-A6-2=]. Adopters can declare their system is a high-risk implementation of one of these specific clauses by using the appropriate SKOS relations.</p>
    <aside class="example" title="Specifying a high-risk AI system according to Annex III-3(a)">
      <p>Annex III-3(a) concerns "AI systems intended to be used to determine access or admission or to assign natural persons to educational and vocational training institutions at all levels". These are represented by the concept [=HighRiskAISystem-AnnexIII-3-a=].</p>
      <pre><code>ex:MySystem a eu-aiact:HighRiskAISystem ;
  skos:broader eu-aiact:HighRiskAISystem-AnnexIII-3-a .</code></pre>
    </aside>
    <p>In addition to represent each clause of the specific annexes as an instance of [=HighRiskAISystem=], future work involves also modelling the specifics of the criteria to assist detection of when an AI system is high-risk. For this, each clause, for example in Annex III, is expressed as an [=AISystem=] with additional metadata about its purpose, data involved, and so on. This represents a 'pattern' that can be checked, for example using [[[SHACL]]], to identify which AI systems are high-risk and under which clause in Annex III. Below is a list of relevant concepts identified for modelling each high-risk AI system defined in Annex III.</p>
    <ul>
      <li><strong>Anx.III-1a</strong> - IdentityVerification</li>
      <li><strong>Anx.III-1b</strong> - n/a</li>
      <li><strong>Anx.III-1c</strong> - n/a</li>
      <li><strong>Anx.III-2</strong> - CriticalInfrastructureManagement, CriticalEnergyManagement, CriticalElectricityManagement, CriticalGasManagement, CriticalWaterManagement, CriticalHeatingManagement, CriticalDigitalInfrastructureManagement</li>
      <li><strong>Anx.III-3a</strong> - EducationAccessManagement, EducationStudentAdmissionManagement, EducationStudentAllocation</li>
      <li><strong>Anx.III-3b</strong> - EducationStudentLearningOutcomeAssessment</li>
      <li><strong>Anx.III-3c</strong> - EducationStudentAcademicPotentialAssessment, EducationStudentLearningNeedsAssessment</li>
      <li><strong>Anx.III-3d</strong> - EducationStudentProctoring</li>
      <li><strong>Anx.III-4a</strong> - JobApplicantRecruitment, JobApplicantSelection, TargetedJobAdvertising, JobApplicationAnalysis, JobApplicationScreening, JobInterviewAssessment</li>
      <li><strong>Anx.III-4b</strong> - PersonnelPromotionManagement, PersonnelTerminationManagement, PersonnelBehaviourMonitoring, PersonnelWorkloadManagement, PersonnelPerformanceMonitoring</li>
      <li><strong>Anx.III-5a</strong> - PublicServiceEligibilityAssessment, SocialWelfareEligibilityAssessment, HealthcareEligibilityAssessment, PublicServiceDistributionManagement, SocialWelfareDistributionManagement</li>
      <li><strong>Anx.III-5b</strong> - CreditChecking, MaintainCreditCheckingDatabase, FinancialFraudManagement</li>
      <li><strong>Anx.III-5c</strong> - InsuranceRiskAssessment, InsurancePricingManagement, HealthInsuranceManagement, LifeInsuranceManagement </li>
      <li><strong>Anx.III-5d</strong> - EmergencyResponseEligibilityAssessment, EmergencyResponseCommunicationManagement, EmergencyResponseTriage, EmergencyResponseDispatchManagement, HealthcareEmergencyCareTriage</li>
      <li><strong>Anx.III-6a</strong> - CrimeVictimPrediction</li>
      <li><strong>Anx.III-6b</strong> - LegalInterrogation</li>
      <li><strong>Anx.III-6c</strong> - LegalEvidenceEligibilityAssessment, CrimeInvestigation, LegalOffenceProsecution</li>
      <li><strong>Anx.III-6d</strong> - LegalOffencePrediction, LegalRepeatOffencePrediction</li>
      <li><strong>Anx.III-6e</strong> - CrimeDetection, CrimeInvestigation, CrimeProsecution</li>
      <li><strong>Anx.III-7a</strong> - LegalInterrogation</li>
      <li><strong>Anx.III-7b</strong> - ImmigrationSecurityManagement, ImmigrationHealthRiskManagement, UnauthorisedMigrationManagement</li>
      <li><strong>Anx.III-7c</strong> - ImmigrationEligibilityAssessment, ImmigrationApplicationManagement, ImmigrationRequestManagement, ImmigrationVisaManagement, ImmigrationResidencyPermitManagement, ImmigrationAsylumManagement, ImmigrationComplaintManagement</li>
      <li><strong>Anx.III-7d</strong> - ImmigrationAsylumManagement, ImmigrationMigrationManagement, ImmigrationBorderControlManagement, ImmigrationIdentityVerification, ImmigrationDocumentVerification</li>
      <li><strong>Anx.III-8a</strong> - JudicialLegalInterpretation, JudicialDisputeManagement</li>
      <li><strong>Anx.III-8b</strong> - PoliticalCampaign</li>
    </ul>
    {{ list_hierarchy(modules['highrisk-systems']['classes']) }}
  </section>

  <section id="vocab-prohibited-systems">
    <h3>Prohibited AI Systems</h3>
    <p>The concept [=ProhibitedAISystem=] represents [=AISystem=] that are prohibited under the AI Act's Article 5. Instances of these concepts are provided for each of the clauses in Article 5, with the relation [=hasRiskLevel=] [=RiskLevelProhibited=] to also indicate their risk level as prohibited.</p>
    <aside class="note" title="Work in progress">
      <p>This section reflects the work in progress for expressing specific prohibitions described in Article 5 using DPV concepts. We welcome help/support for this.</p>
      <p>Planned future work includes potential modelling of systems that are excluded from being prohibited in Article 5(5), to model each prohibited system in terms of its specific concepts such as purposes, outcomes, or types of AI capabilities and systems.</p>
    </aside>
    {{ list_hierarchy(modules['prohibited-systems']['classes']) }}
  </section>

</section>

<section id="FRIA">
  <h2>FRIA</h2>
   <aside class="note" title="Work in progress">
      <p>This section reflects the work in progress for providing concepts and support the Fundamental Rights Impact Assessments (FRIA) as described in Article 27 using DPV concepts. See the articles <a href="https://harshp.com/research/publications/074-FRIA-Tool-DPIA">Towards An Automated AI Act FRIA Tool That Can Reuse GDPR's DPIA</a> and <a href="https://harshp.com/research/publications/075-FRIA-Ontology"> Developing an Ontology for AI Act Fundamental Rights Impact Assessments</a> for more details on ongoing work. We welcome help/support for this.</p>
    </aside>
</section>

<section id="AI-incident">
  <h2>AI Incidents &amp; Future High-Risk Categorisations</h2>
  <aside class="note" title="Work in progress">
    <p>Article 7 of the AI Act defines the criteria for use-cases to be considered as high-risk AI systems in future amendments to Annex III. The DPVCG welcomes discussions and contributions that use this criteria to support this work by modelling AI systems and AI incidents through existing and additional concepts. In particular, we are interested in how assessment of an AI system's characteristics and impacts can be represented using the DPV's vocabularies, and how this can be used to document which systems warrant investigation towards a high-risk categorisation.</p>
  </aside>
</section>

<section id="vocabulary">
<h2>Vocabulary Index</h2>
  <section id="dpv-classes">
    {{ index_concepts(vocab, vocab_name, filter="classes") }}
  </section>
  <section id="dpv-properties">
    {{ index_concepts(vocab, vocab_name, filter="properties") }}
  </section>
  <section id="external-concepts">
    <p>DPV uses the following terms from [[RDF]] and [[RDFS]] with their defined meanings:</p>
    <ul>
      <li id="rdf:type"><dfn>rdf:type</dfn> to denote a concept is an instance of another concept</li>
      <li id="rdfs:Class"><dfn>rdfs:Class</dfn> to denote a concept is a Class or a category</li>
      <li id="rdfs:subClassOf"><dfn>rdfs:subClassOf</dfn> to specify the concept is a subclass (subtype, sub-category, subset) of another concept</li>
      <li id="rdf:Property"><dfn>rdf:Property</dfn> to denote a concept is a property or a relation</li>
      </ul>
    <p>The following external concepts are re-used within DPV:</p>
    {{ index_concepts(vocab, vocab_name, filter="external") }}
  </section>
</section>
{% endblock VOCAB %}

<section id="future-work" class="appendix">
  <h2>Future Work</h2>
  <aside class="issue" data-number="199"></aside>
  <aside class="issue" data-number="229"></aside>
  <aside class="issue" data-number="230"></aside>
  <aside class="issue" data-number="261"></aside>
  <aside class="issue" data-number="281"></aside>
  <aside class="issue" data-number="282"></aside>
  <aside class="issue" data-number="300"></aside>
</section>

{% block ACKNOWLEDGEMENTS %}
<section id="funding-acknowledgements" class="notoc">
  <h2>Funding Acknowledgements</h2>

  <h3>Funding Sponsors</h3>
  {{ funding_acknowledgements() }}

  <h3>Funding Acknowledgements for Contributors</h3>
  <p>The contributions of Delaram Golpayegani have received funding through the <a href="https://protect-network.eu/">PROTECT ITN Project</a> from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 813497, in particular through the development of <a href="https://w3id.org/airo">AI Risk Ontology (AIRO)</a> and <a href="https://w3id.org/vair">Vocabulary of AI Risks (VAIR)</a> which have been integrated in to this extension.</p>
  <p>The contributions of Harshvardhan J. Pandit have been made with the financial support of Science Foundation Ireland under Grant Agreement No. 13/RC/2106_P2 at the ADAPT SFI Research Centre; and the AI Accountability Lab (AIAL) which is supported by grants from following groups: the AI Collaborative, an Initiative of the Omidyar Group; Luminate; the Bestseller Foundation; and the John D. and Catherine T. MacArthur Foundation.</p>

</section>
{% endblock ACKNOWLEDGEMENTS %}

{% block PROPOSED %}
{% if proposed %}
<section id="proposed-terms" class="appendix">
  <h2>Proposed Terms</h2>
  <p>The following terms have been proposed for inclusion, and are under discussion. They are provided here for illustrative purposes and should not be considered as part of DPV.</p>
  {% for name, terms in proposed.items() %}
    <strong>{{name}}</strong>
    <ul>{% for term in terms %}
      <li>{{term}}</li>
    {% endfor %}</ul>
  {% endfor %}
</section>
{% endif %}
{% endblock PROPOSED %}
<section class="appendix" id="issue-summary"></section>

<section id="changelog">
  <h2>Changelog for v{{DPV_VERSION}}</h2>
  <p><strong>total terms: 125 ; added: 15 ; removed: 5 </strong></p>
  <p>The <a href="../changelog.html">changelog</a> provides more information on concepts that have been added/removed in this version. Below is a summary of the changes.</p>
  <p>Removed concepts represent changing wording of the concepts (e.g. `Biometric` to `BiometricData`) and fixing typos. Variants have been added with proper naming, so there are no net concepts removed.</p>
  <p>Added Risk Levels for High (with further Annex I and III), Not High, Permitted, Prohibited, Transparency Required, and Minimal.</p>
</section>

<script type="text/javascript" src="../../../diagrams/common.js" defer></script>
</body>
</html>
