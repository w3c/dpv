{% from 'macro_term_table.jinja2' import table_classes %}
{% from 'macro_term_table.jinja2' import table_properties %}
{% from 'macro_term_table.jinja2' import list_hierarchy, index_concepts, anchor %}
{% from 'macro_dpv_document_family.jinja2' import dpv_document_family, sotd, funding_acknowledgements, contributors_list, authors_list %}

{% macro make_risk_table(concepts, head) %}
<table class="sortable">
  <thead>
    <tr>
      <th>Concept</th>
      <th colspan="4">Roles</th>
      <th colspan="3">CIA model</th>
    </tr>
    <tr>
      <th></th>
      <th>Risk Source</th>
      <th>Risk</th>
      <th>Consequence</th>
      <th>Impact</th>
      <th>Confidentiality</th>
      <th>Integrity</th>
      <th>Availability</th>
    </tr>
  </thead>
  <tbody>
    {% for term, data in concepts.items()|sort(attribute='0') %}
    <tr>
      <td><a href="{{data['iri']}}" class="local-link">{{ term }}</a></td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:PotentialRiskSource') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:PotentialRisk') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:PotentialConsequence') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:PotentialImpact') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:ConfidentialityConcept') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:IntegrityConcept') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:AvailabilityConcept') %}&check;{% endif %}</td>
    </tr>
    {% endfor %}
  </tbody>
</table>
{% endmacro %}

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Technology Concepts</title>
  <script src="https://www.w3.org/Tools/respec/respec-w3c" class="remove" defer></script>
  <script class="remove">
   // All config options at https://respec.org/docs/ 
   var respecConfig = {
    shortName: "AI",
    title: "{{data[vocab_name+'-metadata']['dct:title']}} ({{vocab_name|upper}})",
    subtitle: "version {{data[vocab_name+'-metadata']['schema:version']}}",
    publishDate: "{{data[vocab_name+'-metadata']['dct:modified']}}",
    specStatus: "{{vocab_name|get_document_status}}",
    group: "dpvcg",
    latestVersion: "https://w3id.org/dpv/ai",
    canonicalUri: "https://w3id.org/dpv/ai",
    edDraftURI: "https://dev.dpvcg.org/ai",
    github: "w3c/dpv",
    subjectPrefix: "[AI]",
    doJsonLd: true,
    lint: { "no-unused-dfns": false },
    editors: [
    {
      name: "Harshvardhan J. Pandit",
      url: "https://harshp.com",
      "company": "{{ "Harshvardhan J. Pandit" | generate_author_affiliation }}"
    }
    ],
    authors: {{ authors_list(data, vocab_name) }},
    otherLinks: [
      {
        "key": "This Release",
        "data": [
            {
              "value": "https://w3id.org/dpv/{{DPV_VERSION}}/ai",
              "href": "https://w3id.org/dpv/{{DPV_VERSION}}/ai"
            }
        ]
      },
      {
        "key": "Previous Release",
        "data": [
            {
              "value": "https://w3id.org/dpv/{{DPV_PREVIOUS_VERSION}}/ai",
              "href": "https://w3id.org/dpv/{{DPV_PREVIOUS_VERSION}}/ai"
            }
        ]
      },
      {
        "key": "Changelog",
        "data": [
            {
              "value": "Changelog for v{{DPV_VERSION}}",
              "href": "#changelog"
            }
        ]
      },
      {
        "key": "Key Publications",
        "data": [
            {
              "value": "Data Privacy Vocabulary (DPV) -- Version 2.0 (2024)",
              "href": "https://doi.org/10.1007/978-3-031-77847-6_10"
            },
            {
              "value": "To Be High-Risk, or Not To Be—Semantic Specifications and Implications of the AI Act’s High-Risk AI Applications and Harmonised Standards (2023)",
              "href": "https://doi.org/10.1145/3593013.3594050"
            },
            {
              "value": "AIRO: an Ontology for Representing AI Risks based on the Proposed EU AI Act and ISO Risk Management Standards (2022)",
              "href": "https://doi.org/10.3233/SSW220008"
            }
        ]
      }
    ],
    localBiblio: {%  include 'references.json' %}
  };
</script>
<link rel="stylesheet" type="text/css" href="../diagrams/common.css">
<link rel="shortcut icon" href="../diagrams/favicon-16x16.png" type="image/x-icon" sizes="16x16" />
  <link rel="shortcut icon" href="../diagrams/favicon-32x32.png" type="image/x-icon" sizes="32x32" />
</head>
<body>
  {{ contributors_list(data, vocab_name) }}
  <section id="abstract">
    <p>The AI extension extends the [[[DPV]]] and its [[[TECH]]] extension to represent AI techniques, applications, risks, and mitigations. The namespace for terms in <code>ai</code> is <a href="http://www.w3id.org/dpv/ai#"><code>https://www.w3id.org/dpv/ai#</code></a>. The suggested prefix for the namespace is <code>ai</code>. The AI vocabulary and its documentation are available on <a href="https://github.com/w3c/dpv">GitHub</a>.</p>
  </section>
    {{ sotd(DPV_VERSION=DPV_VERSION, metadata=data[vocab_name+'-metadata']) }}
    {{ dpv_document_family(document='ai-spec') }}

  <section id="vocab-core">
    <h2>Core Concepts</h2>
    <figure>
      <img src="../diagrams/ai.svg">
      <figcaption>Overview of AI extension</figcaption>
    </figure>
    <p>The [[[AI]]] extension further extends the [[TECH]] extension to represent concepts specifically associated with development, use, and operation of AI, and provides:</p>
    <ul>
      <li><strong>Techniques</strong> such as machine learning and natural language programming</li>
      <li><strong>Capabilities</strong> such as image recognition and text generation</li>
      <li><strong>AI Systems and Models</strong> such as expert systems, or general purpose AI models (GPAI)</li>
      <li><strong>Data</strong> such as for training, testing, and validation</li>
      <li><strong>Development Phases</strong> such as training</li>
      <li><strong>Risks</strong> such as data poisoning, statistical noise and bias, etc.</li>
      <li><strong>Risk Measures</strong> to address the AI specific risks</li>
      <li><strong>Lifecycle</strong> such as data collection, training, fine-tuning, etc.</li>
      <li><strong>Documentation</strong> such as Datasheets and Model Cards</li>
      <li><strong>Actors</strong> such as AI Developer and AI Deployer</li>
      <li><strong>Status</strong> associated with AI development</li>
    </ul>
    <p>The following sources have been used to model the concepts defined in this specification:</p>
      <ol>
        <li>[[[ISO-22989]]] -- this standard defines the terminology for AI and represents an international consensus.</li>
        <li><a href="https://www.iso.org/standard/77304.html">ISO/IEC 23894:2023 Artificial Intelligence — Guidance on Risk Management</a></li>
        <li>[[[AIAct]]] -- as the world's first regulation to govern Artificial Intelligence, the AI Act's terminologies and definitions have been instrumental in identifying which concepts should be modelled and to develop taxonomies for their detailed representation. The [[EU-AIAct]] extension defines the concepts explicitly linked/derived from the law by building on the general AI concepts provided in this specification.</li>
        <li>[[[EUVOC-AI-Taxonomy]]] is an authoritative taxonomy of AI concepts provided by the Publications Office of the European Union</li>
        <li><a href="https://ai-watch.ec.europa.eu/publications/defining-artificial-intelligence-10_en">AI Watch taxonomy</a></li>
        <li>[[[AIRO]]] and [[[VAIR]]], which represent comprehensive research-based artefacts to represent AI and AI Act related concepts, have been incorporated into the DPV vocabularies.</li>
      </ol>
      <p>The following sources represent potential sources for future concepts:</p>
      <ol>
        <li><a href="https://www.eit.europa.eu/sites/default/files/creation_of_a_taxonomy_for_the_european_ai_ecosystem_final.pdf">CREATION OF A TAXONOMY FOR THE EUROPEAN AI ECOSYSTEM: A report of the Cross-KIC Activity “Innovation Impact Artificial Intelligence”</a> provides an overview of taxonomies used in the context of AI, and provides an harmonised or aligned view of them to evaluate the different perspectives on AI terminologies. While the report is comprehensive, it does not provide clear definitions or criteria for how concepts are modelled, and therefore is being considered as a potential source of concepts for future enhancements to the DPV's AI extension.</li>
        <li><a href="https://www.iso.org/standard/88145.html">ISO/IEC 22989:2022/DAmd 1 Artificial intelligence concepts and terminology Amendment 1: Generative AI</a></li>
      </ol>

    <section>
      <h3>AI as a specific Technology</h3>
      <p>Artificial Intelligence (AI) is a category of Technology that exhibits or satisfies specific behaviour. While the exact definition of what constitutes 'AI' continues to be a subject of debate and regulation, we focus on the generally understood use of 'AI technologies' and thereby provide this extension to represent information about them in terms of developing and using them, and describing other relevant information about AI such as the specific risks involved and relevant mitigations and measures, documentation, involved data, and a description of the underlying technology itself in terms of specific operations and functions. As there is no consistent vocabulary or standard which is used uniformly within this domain, the concepts provided in this extension represent the specific way the DPVCG has chosen to represent information about AI technologies.</p>
      <p>The AI extension is based on the modelling of technologies in DPV vocabularies. For this reason, it extends the [[TECH]] extension, and only provides AI-specific concepts in this extension. For example, the entity that is the developer of an AI system is represented by the same concept as the developer of any technology through the `tech:Developer` concept. If and when we identify AI-specific actors and roles, those will be defined in this extension by extending the relevant DPV and TECH entities.</p>
    </section>

    <section>
      <h3>Conceptual Model</h3>
      <figure>
        <img src="../diagrams/ai_conceptual_model.svg" />
        <figcaption>Overview of the conceptual model for how AI is described as a technology in DPV and AI extension. The notes provide an example showing how the process of unlocking a phone for identity authentication is described using DPV, and the details of how this functions at a technical level is provided through the concepts in AI extension.</figcaption>
      </figure>
      <p>The concept [=AI=] and its corresponding relation [=hasAI=] represent the broad and generic concept of 'AI' and its use in different contexts. For example, AI might be used to refer to a specific technical algorithm (e.g. conventional computer science use of AI), or a way of automating specific tasks (e.g. business process use of AI), or to describe a process where AI is used in part (e.g. marketing use of AI). To explicitly and accurately describe what is involved in 'AI', we provide further granular additional concepts based on a 'three-layer approach' consisting of [=Technique=] and [=Capability=] for describing the technical implementation and goals, and 'Purpose' (represented by `dpv:Purpose`) for describing the broader aim of the process.</p>
      <p>[=Technique=] represents the underlying 'technique' or 'algorithm', for example [=MachineLearning=] or its specific forms [=NeuralNetwork=] and [=SupervisedLearning=]. It is a technical detail that does not have a specific goal or purpose in the implementation, and which is applied in different contexts to achieve different outcomes.</p>
      <p>[=Capability=] refers to the use of a technique to achieve or perform a (technical) goal or objective. It describes what the technology is 'capable' of doing in terms of a 'technical goal'. For example, [=FaceRecognition=] is a capability for using some underlying [=Technique=] to achieve its goal of recognising faces. However, by itself, we still don't know why facial recognition is being used or developed within the process. This is where `dpv:Purpose` then describes the broader goal or aim for not just the use of AI but also other contextual information such as data, people, entities - such as to state this is being done for identity verification and enforcement of security.</p>
      <p>The separation of concepts in this manner also allows for an efficient and accurate representation of how AI technologies are developed and applied in practice. For example, _Entity1_ develops a algorithmic framework to ingest data and perform some statistical operations on it - this is represented as a [=Technique=]. This framework is then taken by _Entity2_ who uses it towards generating content - this is represented as a [=Capability=]. It then puts this on the market as a product. _Entity3_ then uses this product to provide a service to its customers in terms of recommendations - this is represented as a `dpv:Purpose`. In its knowledge graph, _Entity3_ records that it uses a technology with the relevant AI capability, while the knowledge graph of _Entity2_ represents that it uses the framework produced by _Entity1_.</p>
    </section>

  </section>

  <section id="vocab-techniques">
    <h2>Techniques</h2>
    <p>[=Technique=] represents the underlying technical implementation, and is associated using [=hasTechnique=]. It represents the lowest level of technical detail within the conceptual model used in this extension to describe 'AI technology'. It is useful to describe how the AI technology works in terms of specific algorithms and methodologies being used. By itself, a technique is not sufficient to describe what the AI technology is being used for, i.e. [=Technique=] is distinct from [=Capability=] and {{anchor('dpv:Purpose')}}.</p>
    <p>An implementation of AI technology can be developed only based on a technique - for example as a library or as a framework that can be reused by others. Therefore, a technique can act as a component of a larger AI system where it represents a particular method for implementing something. A technique can also involve the use of other techniques in a composite or combined manner.</p>
    {{ list_hierarchy(modules['techniques']['classes']) }}
  </section>

  <section id="vocab-capabilities">
    <h2>Capabilities</h2>
    <p>[=Capability=] represents the use of a [=Technique=] to achieve a particular technical goal or objective, which is indicated using the [=hasCapability=] relation. In this sense, it forms the middle level of technical detail within the conceptual model used in this extension. By itself, a capability only describes a 'goal' or 'purpose' that is limited to the technical context i.e. what the technology is trying to do or perform. This is distinct from the goal or purpose of the process within which the AI technology is used, which forms the highest level of detail. This distinction allows clarity on what is the actual purpose of an activity in the sense of a high-level or broad goal (expressed using {{ anchor('dpv:Purpose') }}) distinct and separate from the specific reason for using the technology towards achieving this goal.</p>
    <aside class="note" title="Distinction between ai:Technique, ai:Capability, dpv:Purpose, and tech:Function.">
      <p>The primary distinction between these concepts is that {{anchor('ai:Technique')}} and {{anchor('ai:Capability')}} are specific to AI, {{anchor('tech:Function')}} concerns any technology, whereas {{anchor('dpv:Purpose')}} is generic. Additionally, `Technique` refers to how the AI is developed, whereas `Capability` and `Purpose`, by contrast, is limited to refer to end-goals or broad justifications not just for the technology but rather in order to understand the motivations and intentions of the entities involved. `Function` is a 'catch-all' concept to express what a technology is capable of in a manner that can include techniques, capabilities, and purposes, but can also go beyond these to include aspects such as data collection methods and security measures. In DPV, the more specific concepts should be used first (i.e. `Technique`, `Capability`, and `Purpose`) and where information cannot be defined using these, a broad concept such as `Function` is provided.</p>
      <p>The following examples further outline the distinctions between [=Technique=], [=Capability=], and {{anchor('dpv:Purpose')}}.</p>
      <ol>
        <li>A smartphone uses speech recognition to recognise commands and learns to better identify the user's voice over time through the use of deep learning. Here, the purpose is not specified as to what's the end goal of doing this, while speech recognition is the capability used with deep learning as a technique.</li>
        <li>A CCTV camera monitors footage in a high-security building and performs facial recognition to ensure unknown persons are flagged. Here, maintaining security is the purpose and facial recognition is a capability used to achieve it.</li>
        <li>An organisation tracks a person across the internet and profiles them based on a combination of neural networks and semi-supervised learning. Here, 'profiling' by itself is a capability but its purpose i.e. why is it being done is not specified, while the techniques (neural network and semi-supervised learning) used to achieve it are specified.</li>
      </ol>
    </aside>
    <p>As with techniques, specific software and services can provide capabilities for inclusion in an AI system or technology. For example, [=ImageRecognition=] can be integrated in an AI system through an API through an API service, or [=NamedEntityRecognition=] can be implemented by using software libraries. This implies that AI technologies need not be developed with specific capabilities, and that such capabilities can be added or combined later within the AI system.</p>
    <aside class="note" title="Capabilities do not imply authenticity or validity of methods">
      <p>A [=Capability=] does not necessarily imply that the AI technology is indeed capable of performing what is described, as such capabilities can also be identified through marketing claims or misuse of a technology. For example, DPV's taxonomy contains capabilities such as [=EmotionRecognition=] which might be implemented through a [=TrainedModel=]. However, even if the model somehow returns a high statistical accuracy for a given sample data, such capabilities still risk being inaccurate and incomplete due to their basis in human physiology and psychology.</p>
      <p>Regulations recognise this risk, for example the AI Act's Recital 44 notes emotional recognition risks as "limited reliability, the lack of specificity and the limited generalisability" and that they "may lead to discriminatory outcomes and can be intrusive to the rights and freedoms". For this reason, the AI Act Article 5-1f prohibits this practice in workplaces and educational institutions. We are interested in capturing such concerns through the <a href="#vocab-risks">AI-specific Risks section</a> in this extension.</p>
    </aside>
    {{ list_hierarchy(modules['capabilities']['classes']) }}
  </section>

  <section id="vocab-systems">
    <h2>AI Systems and Models</h2>
    <p>[=AISystem=] is defined by ISO/IEC 22989:2023 as "An engineered system that generates outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives", and by OECD as "A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment." Or simply, it represents a 'system' which uses 'AI technologies'.</p>
    <p>The property [=hasAISystem=] associates the use of an AI system in context. It is a specialised form of `dpv:isImplementedUsingTechnology` which indicates that a process is being implemented through the use of the stated technology. The components of an AI system can be described through the use of concepts provided in this ([[AI]]) extension as well as through the [[TECH]] extension.</p>
    <p>[=Model=] is defined as "a physical, mathematical or otherwise logical representation of a system, entity, phenomenon, process or data involving the use of AI techniques". Or simply, it represents a 'model' of something using 'AI technologies'. The property [=hasModel=] associates a model with a context, such as to indicate a particular AI system utilises the specified model. To specifically represent General-Purpose AI (GPAI) models, the concept [=GPAIModel=] and relation [=hasGPAIModel=] are provided.</p>
    <p>The below taxonomy provides additional concepts based on categorisation of [=AISystem=] and [=Model=] in different contexts.</p>
    {{ list_hierarchy(modules['systems']['classes']) }}
  </section>

  <section id="vocab-ai-agent">
    <h2>AI Agents</h2>
    <p>The concept [=AIAgent=] represent the generic notion of 'AI agents' which are software that represent an entity or act on their behalf.</p>
    <aside class="note" title="Contributions welcome for enhancement of AI Agent concepts">
      <p>The DPVCG is interested in the modelling of 'Agents' in the context of AI technologies, including how they interact with systems and other agents, and how they operate through the use of AI technologies. For this, we welcome proposals and participation.</p>
    </aside>
  </section>

  <section id="vocab-data">
    <h2>Data</h2>
    <p>The concept [=Data=] is a broad generic term for describing data involved in the context of AI technology. At the moment, this includes three categories - [=TrainingData=], [=ValidationData=], and [=TestingData=]. The DPVCG welcomes proposals and participation to further enhance this taxonomy.</p>
    <p>[=Data=] extends {{anchor('dpv:Data')}}, and can be associated with the property [=hasData=] which is a specialised form of {{anchor('dpv:hasData')}} to indicate the specified data is involved in context of an AI technology. To specifically indicate the contextual involvement of data within AI development, the properties [=hasTrainingData=], [=hasTestingData=], and [=hasValidationData=] are provided.</p>
    <p>To indicate the involvement of personal data, the concept `dpv:PersonalData` should be used along with its relation `dpv:hasPersonalData`. The [[DPV]] taxonomy contains specific concepts to model sensitive data - including that related to confidential and IP, and the [[PD]] extension provides a taxonomy of personal data categories that can be used to indicate involvement in AI technologies.</p>
    {{ list_hierarchy(modules['data']['classes']) }}
  </section>

  <section id="vocab-development">
    <h2>Model Development Phases</h2>
    <p>[=ModelTraining=] refers to the process through which [=TrainingData=] is transformed to produce a [=TrainedModel=]. [=ModelFineTuning=] represents a further refinement phase where a prior [=TrainedModel=] is further refined using an additional smaller (and usually more targeted) [=TrainingData=] to produce a [=FineTunedModel=]. In these, the concepts for _training_ are defined by extending the generic concept of `Processing` in [[DPV]] to represent the processing of data.</p>
    <p>As part of the development and training of models, data usually undergoes several key steps. These are represented by the concept [=DataOperation=], which involves broadly [=DataCollection=] and [=DataPreparation=], and optionally [=DataAggregation=]. These are also defined by extending relevant processing operations from [[DPV]], for example to represent where they collect, obtain, or transform data.</p>
    {{ list_hierarchy(modules['development']['classes']) }}
  </section>

  <section id="vocab-risks">
    <h2>Risk Concepts</h2>
    <aside class="note" title="AI Risk concepts extend the RISK extension concepts">
      <p>The risk concepts presented in this extension are intended to represent concepts specific to AI development and use, and do not contain general risk concepts which exist in other contexts e.g. applicable for any (AI and non-AI) technology. The [[RISK]] extension contains the general set of concepts that this vocabulary extends to represent risks that are specific to the development and use of AI.</p>
    </aside>
    <p>The concept [=RiskConcept=] in this extension extends {{anchor('dpv:RiskConcept')}} to represent risk sources, risks, consequences, and impacts specific to the development, use, or operation of AI. As with the [[RISK]] extension, the risk concepts presented here can taken on different roles in different use-cases, for example what is a risk source in one scenario could be the consequence in another. The relations <code>risk:hasRiskSource</code>, <code>dpv:hasRisk</code>, <code>dpv:hasConsequence</code>, and <code>dpv:hasImpact</code> are useful to indicate the specific interpretation and role of the AI risk concepts in a scenario.</p>
    <p>The AI Risk Concepts are broadly categorised according to the following:</p>
    <ol>
      <li>[=DataRisk=] - Risk associated with data used or produced or otherwise involved in the context of AI</li>
      <li>[=SecurityAttack=] - Risks or issues associated with security attacks related to AI technologies, models, and systems</li>
      <li>[=ModelRisk=] - Risks associated with AI Models</li>
      <li>[=AISystemRisk=] - Risks associated with AI Systems</li>
      <li>[=UserRisk=] - Risks associated with Users of AI Systems</li>
      <li>[=AIBias=] - Bias associated with development, use, or other activities involving an AI technology or system</li>
    </ol>

    <section id="vocab-risks-data">
      <h3>Data Risks</h3>
      <p>[=DataRisk=] represent risks associated with the data involved in AI technologies. To represent these risks in the context of the role the data is playing (training, testing, validation), the same set of data risks are expressed for each of the three data categories to accurately represent both the origin and occurrence of the risk.</p>
      {{ list_hierarchy(modules['risks']['classes'], head='ai:DataRisk') }}
    </section>

    <section id="vocab-risks-bias">
      <h3>Bias</h3>
      <p>The bias concepts represented here are specific to AI, and there are generic bias concepts as well as discrimination impact concepts in [[RISK]] extension. While we are interested in further expanding these concepts, the following external sources should be of interest:</p>
      <ul>
        <li><a href="https://github.com/tibonto/Doc-BIAS">DocBiasO</a> - an ontology-driven approach to support the documentation of bias in data, which has a larger expansive categorisation of bias and provides additional concepts and properties to model specifics such as ethnicities and measurements which are useful in bias measurement and documentation.</li>
        <li><a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf">NIST Special Publication 1270 - Towards a Standard for Identifying and Managing Bias in Artificial Intelligence</a></li>
        <li><a href="https://www.iso.org/standard/77607.html">ISO/IEC TR 24027:2021 Information technology — Artificial intelligence (AI) — Bias in AI systems and AI aided decision making</li>
        <li><a href="https://oecd.ai/en/catalogue/tools?terms=bias&page=1">OECD AI Policy Observatory - Catalogue of Tools & Metrics for Trustworthy AI</a></li>
      </ul>
      {{ list_hierarchy(modules['risks']['classes'], head='ai:AIBias') }}
    </section>

    <section id="vocab-risks-security-attack">
      <h3>Security Attacks</h3>
      <aside class="note" title="Add content for Security Attack section"></aside>
      {{ list_hierarchy(modules['risks']['classes'], head='ai:SecurityAttack') }}
    </section>

    <section id="vocab-risks-concepts">
      <h3>Overview of Risk Concepts</h3>
      <p>The below table provides suggestions for the role each concept can be used for in the context of risk assessment, and how they can be categorised within the conventional 'CIA security model'. For example, [=AdversarialAttack=] can be used as a risk source (i.e. it can cause further issues to arise), a risk (i.e. it is a risk of concern), or as a consequence (i.e. it can occur due to another risk), and it is classified as affecting 'integrity' in the CIA model.</p>
      <p>This table is based on a similar table within the [[RISK]] extension which provides a detailed taxonomy of concepts and the potential roles they can take across use-cases.</p>
      {{ make_risk_table(modules['risks']['classes']) }}
    </section>

  <section id="vocab-measures">
    <h2>Risk Measures</h2>
    <aside class="note" title="Add content for Risk Measures concept - mention ai:Measure as parent concept and that we are looking to expand this in the future based on DPV TOMs taxonomies specifically for AI"></aside>
    <p>The concept [=Measure=] is a specific measure associated with AI technologies to address risks related to AI technologies. While the [[DPV]] and [[RISK]] extension provide relevance and modelling of measures along with detailed taxonomies, this concept is useful to represent the measures developed and specifically used for AI technologies. The DPVCG welcomes proposals and participation to further expand the taxonomy of measures.</p>
  </section>

</section>

<section id="vocab-lifecycle">
    <h2>Lifecycle</h2>
    <p>[=LifecycleStage=] models the lifecycle of AI technologies from its inception to deployment, use, and retirement. While we use the term 'lifecycle' here, these stages are also useful in other similar contexts such as 'AI Value Chain' and 'AI Supply Chain'. The AI-specific lifecycle is extended from the concept `tech:LifecycleStage` defined in the [[TECH]] extension to model lifecycle and stages of technologies in general. It can therefore be used with the existing relation `tech:hasLifecycleStage` to denote its applicability or involvement.</p>
    <aside class="note" title="Alignment with lifecycle of technology">
      <p>We are currently exploring the alignment of these concepts, which are based on ISO/IEC 22989:2022, with those for lifecycle of technology (in general) as defined in ISO/IEC/IEEE 15288:2023 Systems and software engineering — System life cycle processes. We have proposed inclusion of technology lifecycle concepts in the [[TECH]] extension, which would then be extended here in the AI extension.</p>
    </aside>
    {{ list_hierarchy(modules['lifecycle']['classes']) }}
  </section>
  

  <section id="vocabulary">
<h2>Vocabulary Index</h2>
  <section id="dpv-classes">
    {{ index_concepts(vocab, vocab_name, filter="classes") }}
  </section>
  <section id="dpv-properties">
    {{ index_concepts(vocab, vocab_name, filter="properties") }}
  </section>
  <section id="external-concepts">
    <p>DPV uses the following terms from [[RDF]] and [[RDFS]] with their defined meanings:</p>
    <ul>
      <li id="rdf:type"><dfn>rdf:type</dfn> to denote a concept is an instance of another concept</li>
      <li id="rdfs:Class"><dfn>rdfs:Class</dfn> to denote a concept is a Class or a category</li>
      <li id="rdfs:subClassOf"><dfn>rdfs:subClassOf</dfn> to specify the concept is a subclass (subtype, sub-category, subset) of another concept</li>
      <li id="rdf:Property"><dfn>rdf:Property</dfn> to denote a concept is a property or a relation</li>
      </ul>
    <p>The following external concepts are re-used within DPV:</p>
    {{ index_concepts(vocab, vocab_name, filter="external") }}
  </section>
</section>
  
{% if proposed %}
<section id="proposed-terms" class="appendix">
  <h2>Proposed Terms</h2>
  <p>The following terms have been proposed for inclusion, and are under discussion. They are provided here for illustrative purposes and should not be considered as part of DPV.</p>
    <ul>{% for term in proposed %}
      <li>{{term}}</li>
    {% endfor %}</ul>
</section>
{% endif %}

<section id="future-work" class="appendix">
  <h2>Future Work</h2>
  <aside class="issue" data-number="94"></aside>
  <aside class="issue" data-number="197"></aside>
  <aside class="issue" data-number="281"></aside>
  <aside class="issue" data-number="294"></aside>
  <aside class="issue" data-number="295"></aside>
  <aside class="issue" data-number="315"></aside>
  <aside class="issue" data-number="385"></aside>
  <aside class="issue" data-number="388"></aside>
</section>

{% block ACKNOWLEDGEMENTS %}
<section id="funding-acknowledgements" class="notoc">
  <h2>Funding Acknowledgements</h2>

  <h3>Funding Sponsors</h3>
  {{ funding_acknowledgements() }}

  <h3>Funding Acknowledgements for Contributors</h3>
  <p>The contributions of Delaram Golpayegani have received funding through the <a href="https://protect-network.eu/">PROTECT ITN Project</a> from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 813497, in particular through the development of <a href="https://w3id.org/airo">AI Risk Ontology (AIRO)</a> and <a href="https://w3id.org/vair">Vocabulary of AI Risks (VAIR)</a> which have been integrated in to this extension.</p>
  <p>The contributions of Harshvardhan J. Pandit and Delaram Golpayegani have been made with the financial support of Science Foundation Ireland under Grant Agreement No. 13/RC/2106_P2 at the ADAPT SFI Research Centre. The contributions of Harshvardhan J. Pandit have been made with the AI Accountability Lab (AIAL) which is supported by grants from following groups: the AI Collaborative, an Initiative of the Omidyar Group; Luminate; the Bestseller Foundation; and the John D. and Catherine T. MacArthur Foundation.</p>

</section>
{% endblock ACKNOWLEDGEMENTS %}

<section id="changelog">
  <h2>Changelog for v{{DPV_VERSION}}</h2>
  <p><strong>total terms: 211 ; added: 6 ; removed: 3</strong></p>
  <p>The <a href="../changelog.html">changelog</a> provides more information on concepts that have been added/removed in this version. Below is a summary of the changes.</p>
  <ul>
      <li><code>DataLabelsAndLabellingProcessBias</code> and <code>MissingFeaturesAndLabelsBias</code> were shortened for correctness and consistency, and <code>DecomissionStage</code> was renamed to fix the typo.</li>
      <li>Added <code>ModelTraining</code> to represent models being trained as a processing activity</li>
      <li>Added <code>TrainingTechnique</code> to represent techniques used for training the model, where existing techniques are declared as types of training.</li>
  </ul>
</section>

<script type="text/javascript" src="../diagrams/common.js" defer></script>
</body>
</html>
