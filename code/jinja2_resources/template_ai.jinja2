{% from 'macro_term_table.jinja2' import table_classes %}
{% from 'macro_term_table.jinja2' import table_properties %}
{% from 'macro_term_table.jinja2' import list_hierarchy, index_concepts %}
{% from 'macro_dpv_document_family.jinja2' import dpv_document_family, sotd, funding_acknowledgements, contributors_list, authors_list %}

{% macro make_risk_table(concepts, head) %}
<table class="sortable">
  <thead>
    <tr>
      <th>Concept</th>
      <th colspan="4">Roles</th>
      <th colspan="3">CIA model</th>
    </tr>
    <tr>
      <th></th>
      <th>Risk Source</th>
      <th>Risk</th>
      <th>Consequence</th>
      <th>Impact</th>
      <th>Confidentiality</th>
      <th>Integrity</th>
      <th>Availability</th>
    </tr>
  </thead>
  <tbody>
    {% for term, data in concepts.items()|sort(attribute='0') %}
    <tr>
      <td><a href="{{data['iri']}}" class="local-link">{{ term }}</a></td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:PotentialRiskSource') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:PotentialRisk') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:PotentialConsequence') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:PotentialImpact') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:ConfidentialityConcept') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:IntegrityConcept') %}&check;{% endif %}</td>
      <td>{% if data['rdf:type']|check_rdf_type('risk:AvailabilityConcept') %}&check;{% endif %}</td>
    </tr>
    {% endfor %}
  </tbody>
</table>
{% endmacro %}

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AI Technology Concepts</title>
  <script src="https://www.w3.org/Tools/respec/respec-w3c" class="remove" defer></script>
  <script class="remove">
   // All config options at https://respec.org/docs/ 
   var respecConfig = {
    shortName: "AI",
    title: "{{data[vocab_name+'-metadata']['dct:title']}} ({{vocab_name|upper}})",
    subtitle: "version {{data[vocab_name+'-metadata']['schema:version']}}",
    publishDate: "{{data[vocab_name+'-metadata']['dct:modified']}}",
    specStatus: "{{vocab_name|get_document_status}}",
    group: "dpvcg",
    latestVersion: "https://w3id.org/dpv/ai",
    canonicalUri: "https://w3id.org/dpv/ai",
    edDraftURI: "https://dev.dpvcg.org/ai",
    github: "w3c/dpv",
    subjectPrefix: "[AI]",
    doJsonLd: true,
    lint: { "no-unused-dfns": false },
    editors: [
    {
      name: "Harshvardhan J. Pandit",
      url: "https://harshp.com",
      "company": "{{ "Harshvardhan J. Pandit" | generate_author_affiliation }}"
    }
    ],
    authors: {{ authors_list(data, vocab_name) }},
    otherLinks: [
      {
        "key": "This Release",
        "data": [
            {
              "value": "https://w3id.org/dpv/{{DPV_VERSION}}/ai",
              "href": "https://w3id.org/dpv/{{DPV_VERSION}}/ai"
            }
        ]
      },
      {
        "key": "Previous Release",
        "data": [
            {
              "value": "https://w3id.org/dpv/{{DPV_PREVIOUS_VERSION}}/ai",
              "href": "https://w3id.org/dpv/{{DPV_PREVIOUS_VERSION}}/ai"
            }
        ]
      },
      {
        "key": "Changelog",
        "data": [
            {
              "value": "Changelog for v2.1",
              "href": "#changelog"
            }
        ]
      },
      {
        "key": "Key Publications",
        "data": [
            {
              "value": "Data Privacy Vocabulary (DPV) -- Version 2.0 (2024)",
              "href": "https://doi.org/10.1007/978-3-031-77847-6_10"
            },
            {
              "value": "To Be High-Risk, or Not To Be—Semantic Specifications and Implications of the AI Act’s High-Risk AI Applications and Harmonised Standards (2023)",
              "href": "https://doi.org/10.1145/3593013.3594050"
            },
            {
              "value": "AIRO: an Ontology for Representing AI Risks based on the Proposed EU AI Act and ISO Risk Management Standards (2022)",
              "href": "https://doi.org/10.3233/SSW220008"
            }
        ]
      }
    ],
    localBiblio: {%  include 'references.json' %}
  };
</script>
<link rel="stylesheet" type="text/css" href="../diagrams/common.css">
<link rel="shortcut icon" href="../diagrams/favicon-16x16.png" type="image/x-icon" sizes="16x16" />
  <link rel="shortcut icon" href="../diagrams/favicon-32x32.png" type="image/x-icon" sizes="32x32" />
</head>
<body>
  {{ contributors_list(data, vocab_name) }}
  <section id="abstract">
    <p>The AI extension extends the [[[DPV]]] and its [[[TECH]]] extension to represent AI techniques, applications, risks, and mitigations. The namespace for terms in <code>ai</code> is <a href="http://www.w3id.org/dpv/ai#"><code>https://www.w3id.org/dpv/ai#</code></a>. The suggested prefix for the namespace is <code>ai</code>. The AI vocabulary and its documentation are available on <a href="https://github.com/w3c/dpv">GitHub</a>.</p>
  </section>
    {{ sotd(DPV_VERSION=DPV_VERSION, metadata=data[vocab_name+'-metadata']) }}
    {{ dpv_document_family(document='ai-spec') }}

  <section id="vocab-core">
    <h2>Core Concepts</h2>
    <figure>
      <img src="../diagrams/ai.svg">
      <figcaption>Overview of AI extension</figcaption>
    </figure>
    <p>The [[[AI]]] extension further extends the [[TECH]] extension to represent concepts specifically associated with development, use, and operation of AI, and provides:</p>
    <ul>
      <li><strong>Techniques</strong> such as machine learning and natural language programming</li>
      <li><strong>Capabilities</strong> such as image recognition and text generation</li>
      <li><strong>AI Systems and Models</strong> such as expert systems, or general purpose AI models (GPAI)</li>
      <li><strong>Data</strong> such as for training, testing, and validation</li>
      <li><strong>Risks</strong> such as data poisoning, statistical noise and bias, etc.</li>
      <li><strong>Risk Measures</strong> to address the AI specific risks</li>
      <li><strong>Lifecycle</strong> such as data collection, training, fine-tuning, etc.</li>
      <li><strong>Documentation</strong> such as Data Sheets and Model Cards</li>
      <li><strong>Actors</strong> such as AI Developer and AI Deployer</li>
      <li><strong>Status</strong> associated with AI development</li>
    </ul>
    <p>The AI extension is created based on the following sources:</p>
    <ol>
      <li>[[[ISO-22989]]]</li>
      <li><a href="https://www.iso.org/standard/77304.html">ISO/IEC 23894:2023 Artificial Intelligence — Guidance on Risk Management</a></li>
      <li>[[[AIAct]]], which is also supported by the [[EU-AIAct]] extension for DPV</li>
      <li><a href="https://ai-watch.ec.europa.eu/publications/defining-artificial-intelligence-10_en">AI Watch taxonomy</a></li>
      <li><a href="https://w3id.org/airo">AI Risk Ontology (AIRO)</a> and <a href="https://w3id.org/vair">Vocabulary of AI Risks (VAIR)</a> which have been integrated in DPV v2.1</li>
    </ol>

    <section>
      <h3>AI as a specific Technology</h3>
      <p>Artificial Intelligence (AI) is a category of Technology that exhibits or satisfies specific behaviour. While the exact definition of what constitutes 'AI' continues to be a subject of debate and regulation, we focus on the generally understood use of 'AI technologies' and thereby provide this extension to represent information about them in terms of developing and using them, and describing other relevant information about AI such as the specific risks involved and relevant mitigations and measures, documentation, involved data, and a description of the underlying technology itself in terms of specific operations and functions. As there is no consistent vocabulary or standard which is used uniformly within this domain, the concepts provided in this extension represent the specific way the DPVCG has chosen to represent information about AI technologies.</p>
      <p>The AI extension is based on the modelling of technologies in DPV vocabularies. For this reason, it extends the [[TECH]] extension, and only provides AI-specific concepts in this extension. For example, the entity that is the developer of an AI system is represented by the same concept as the developer of any technology through the `tech:Developer` concept. If and when we identify AI-specific actors and roles, those will be defined in this extension by extending the relevant DPV and TECH entities.</p>
    </section>

    <section>
      <h3>Conceptual Model</h3>
      <figure>
        <img src="../diagrams/ai_conceptual_model.svg" />
        <figcaption>Overview of the conceptual model for how AI is described as a technology in DPV and AI extension. The notes provide an example showing how the process of unlocking a phone for identity authentication is described using DPV, and the details of how this functions at a technical level is provided through the concepts in AI extension.</figcaption>
      </figure>
      <p>The concept [=AI=] and its corresponding relation [=hasAI=] represent the broad and generic concept of 'AI' and its use in different contexts. For example, AI might be used to refer to a specific technical algorithm (e.g. conventional computer science use of AI), or a way of automating specific tasks (e.g. business process use of AI), or to describe a process where AI is used in part (e.g. marketing use of AI). To explicitly and accurately describe what is involved in 'AI', we provide further granular additional concepts based on a 'three-layer approach' consisting of [=Technique=] and [=Capability=] for describing the technical implementation and goals, and 'Purpose' (represented by `dpv:Purpose`) for describing the broader aim of the process.</p>
      <p>[=Technique=] represents the underlying 'technique' or 'algorithm', for example [=MachineLearning=] or its specific forms [=NeuralNetwork=] and [=SupervisedLearning=]. It is a technical detail that does not have a specific goal or purpose in the implementation, and which is applied in different contexts to achieve different outcomes.</p>
      <p>[=Capability=] refers to the use of a technique to achieve or perform a (technical) goal or objective. It describes what the technology is 'capable' of doing in terms of a 'technical goal'. For example, [=FaceRecognition=] is a capability for using some underlying [=Technique=] to achieve its goal of recognising faces. However, by itself, we still don't know why facial recognition is being used or developed within the process. This is where `dpv:Purpose` then describes the broader goal or aim for not just the use of AI but also other contextual information such as data, people, entities - such as to state this is being done for identity verification and enforcement of security.</p>
      <p>The separation of concepts in this manner also allows for an efficient and accurate representation of how AI technologies are developed and applied in practice. For example, _Entity1_ develops a algorithmic framework to ingest data and perform some statistical operations on it - this is represented as a [=Technique=]. This framework is then taken by _Entity2_ who uses it towards generating content - this is represented as a [=Capability=]. It then puts this on the market as a product. _Entity3_ then uses this product to provide a service to its customers in terms of recommendations - this is represented as a `dpv:Purpose`. In its knowledge graph, _Entity3_ records that it uses a technology with the relevant AI capability, while the knowledge graph of _Entity2_ represents that it uses the framework produced by _Entity1_.</p>
    </section>
  </section>

  <section id="vocab-techniques">
    <h2>Techniques</h2>
    <p>[=Technique=] represents the underlying technical implementation, and is associated using [=hasTechnique=]. It represents the lowest level of technical details within the conceptual model used in this extension to describe 'AI technology'. By itself, a technique is not sufficient to describe what the AI technology is being used for, but it is useful to express how the AI technology functions.</p>
    <p>An implementation of AI technology can be developed only based on a technique - for example as a library or as a framework that can be reused by others. Therefore, a technique can act as a component of a larger AI system where it represents a particular method for implementing something. A technique can also involve the use of other techniques in a composite or combined manner.</p>
    {{ list_hierarchy(modules['techniques']['classes']) }}
  </section>

  <section id="vocab-capabilities">
    <h2>Capabilities</h2>
    <p>[=Capability=] represents the use of a technique to achieve some technical goal or objective, and is associated using [=hasCapability=]. It represents the middle level of technical details within the conceptual model used in this extension to describe 'AI technology'. By itself, a capability it not useful to describe what the end-goal of using the AI technology is, but it is useful to describe what the AI technology is used for in context of achieving an end-goal.</p>
    <p>An implementation of AI technology can be developed with a capability - for example as a service or as a software, which can be used in a stand-alone manner or be integrated in to a larger AI system. Therefore, AI capabilities can occur as both components and systems, and be involved in processes directly or indirectly in this manner. A capability can also involve the use of other capabilities in a composite or combined manner.</p>
    <aside class="note" title="Distinction between Capability and Purpose">
      <p>As outlined earlier, the 'Capability' concept only represents the technical goal i.e. describing what the AI technology is achieving in a technical sense. By itself, it is not sufficient to describe <i>why</i> it is being used - for which the concept `dpv:Purpose` exists. However, despite this, there may be cases where a Capability is expressed without a purpose, such as to describe the development of specific capabilities, which may later be used in a particular use-case towards a specific purpose.</p>
    </aside>
    {{ list_hierarchy(modules['capabilities']['classes']) }}
  </section>

  <section id="vocab-systems">
    <h2>AI Systems and Models</h2>
    <p>[=AISystem=] is defined by ISO/IEC 22989:2023 as "An engineered system that generates outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives", and by OECD as "A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment." Or simply, it represents a 'system' which uses 'AI technologies'.<p>
    <p>The property [=hasAISystem=] associates the use of an AI system in context. It is a specialised form of `dpv:isImplementedUsingTechnology` which indicates that a process is being implemented through the use of the stated technology. The components of an AI system can be described through the use of concepts provided in this ([[AI]]) extension as well as through the [[TECH]] extension.</p>
    <p>[=Model=] is defined as "a physical, mathematical or otherwise logical representation of a system, entity, phenomenon, process or data involving the use of AI techniques". Or simply, it represents a 'model' of something using 'AI technologies'. The property [=hasModel=] associates a model with a context, such as to indicate a particular AI system utilises the specified model. To specifically represent General-Purpose AI (GPAI) models, the concept [=GPAIModel=] and relation [=hasGPAIModel=] are provided.</p>
    <p>The below taxonomy provides additional concepts based on categorisation of [=AISystem=] and [=Model=] in different contexts.</p>
    {{ list_hierarchy(modules['systems']['classes']) }}
    <p>The DPVCG is interested in the modelling of 'Agents' in the context of AI technologies, including how they interact with systems and other agents, and how they operate through the use of AI technologies. For this, we welcome proposals and participation.</p>
    <aside class="issue" data-number="197"></aside>
  </section>

  <section id="vocab-data">
    <h2>Data</h2>
    <p>The concept [=Data=] is a broad generic term for describing data involved in the context of AI technology. At the moment, this includes three categories - [=TrainingData=], [=ValidationData=], and [=TestingData=]. The DPVCG welcomes proposals and participation to further enhance this taxonomy.</p>
    <p>[=Data=] extends `dpv:Data`, and can be associated with the property [=hasData=] which is a specialised form of `dpv:hasData` to indicate the specified data is involved in context of an AI technology. To specifically indicate the contextual involvement of data within AI development, the properties [=hasTrainingData=], [=hasTestingData=], and [=hasValidationData=] are provided.</p>
    <p>To indicate the involvement of personal data, the concept `dpv:PersonalData` should be used along with its relation `dpv:hasPersonalData`. The [[DPV]] taxonomy contains specific concepts to model sensitive data - including that related to confidential and IP, and the [[PD]] extension provides a taxonomy of personal data categories that can be used to indicate involvement in AI technologies.</p>
    {{ list_hierarchy(modules['data']['classes']) }}
  </section>

  <section id="vocab-risks">
    <h2>Risk Concepts</h2>
    <aside class="note" title="AI Risk concepts extend the RISK extension concepts">
      <p>The risk concepts presented in this extension are intended to represent concepts specific to AI development and use, and do not contain general risk concepts which exist in other contexts e.g. applicable for any (AI and non-AI) technology. The [[RISK]] extension contains the general set of concepts that this vocabulary extends to represent risks that are specific to the development and use of AI.</p>
    </aside>
    <p>The concept [=RiskConcept=] in this extension extends <code>dpv:RiskConcept</code> to represent risk sources, risks, consequences, and impacts specific to the development, use, or operation of AI. As with the [[RISK]] extension, the risk concepts presented here can taken on different roles in different use-cases, for example what is a risk source in one scenario could be the consequence in another. The relations <code>risk:hasRiskSource</code>, <code>dpv:hasRisk</code>, <code>dpv:hasConsequence</code>, and <code>dpv:hasImpact</code> are useful to indicate the specific interpretation and role of the AI risk concepts in a scenario.</p>
    <p>The AI Risk Concepts are broadly categorised according to the following:</p>
    <ol>
      <li>[=DataRisk=] - Risk associated with data used or produced or otherwise involved in the context of AI</li>
      <li>[=SecurityAttack=] - Risks or issues associated with security attacks related to AI technologies, models, and systems</li>
      <li>[=ModelRisk=] - Risks associated with AI Models</li>    
      <li>[=AISystemRisk=] - Risks associated with AI Systems</li>    
      <li>[=UserRisk=] - Risks associated with Users of AI Systems</li>    
      <li>[=AIBias=] - Bias associated with development, use, or other activities involving an AI technology or system</li>
    </ol>

    <section id="vocab-risks-data">
      <h3>Data Risks</h3>
      <p>[=DataRisk=] represent risks associated with the data involved in AI technologies. To represent these risks in the context of the role the data is playing (training, testing, validation), the same set of data risks are expressed for each of the three data categories to accurately represent both the origin and occurrence of the risk.</p>
      {{ list_hierarchy(modules['risks']['classes'], head='ai:DataRisk') }}
    </section>

    <section id="vocab-risks-bias">
      <h3>Bias</h3>
      <p>The bias concepts represented here are specific to AI, and there are generic bias concepts as well as discrimination impact concepts in [[RISK]] extension. While we are interested in further expanding these concepts, the following external sources should be of interest:</p>
      <ul>
        <li><a href="https://github.com/tibonto/Doc-BIAS">DocBiasO</a> - an ontology-driven approach to support the documentation of bias in data, which has a larger expansive categorisation of bias and provides additional concepts and properties to model specifics such as ethnicities and measurements which are useful in bias measurement and documentation.</li>
        <li><a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf">NIST Special Publication 1270 - Towards a Standard for Identifying and Managing Bias in Artificial Intelligence</a></li>
        <li><a href="https://www.iso.org/standard/77607.html">ISO/IEC TR 24027:2021 Information technology — Artificial intelligence (AI) — Bias in AI systems and AI aided decision making</li>
        <li><a href="https://oecd.ai/en/catalogue/tools?terms=bias&page=1">OECD AI Policy Observatory -  Catalogue of Tools & Metrics for Trustworthy AI</a></li>
      </ul>
      {{ list_hierarchy(modules['risks']['classes'], head='ai:AIBias') }}
    </section>

    <section id="vocab-risks-security-attack">
      <h3>Security Attacks</h3>
      <aside class="note" title="Add content for Security Attack section"></aside>
      {{ list_hierarchy(modules['risks']['classes'], head='ai:SecurityAttack') }}
    </section>

    <section id="vocab-risks-concepts">
      <h3>Overview of Risk Concepts</h3>
      <p>The below table provides suggestions for the role each concept can be used for in the context of risk assessment, and how they can be categorised within the conventional 'CIA security model'. For example, [=AdversarialAttack=] can be used as a risk source (i.e. it can cause further issues to arise), a risk (i.e. it is a risk of concern), or as a consequence (i.e. it can occur due to another risk), and it is classified as affecting 'integrity' in the CIA model.</p>
      <p>This table is based on a similar table within the [[RISK]] extension which provides a detailed taxonomy of concepts and the potential roles they can take across use-cases.</p>
      {{ make_risk_table(modules['risks']['classes']) }}
    </section>

  <section id="vocab-measures">
    <h2>Risk Measures</h2>
    <aside class="note" title="Add content for Risk Measures concept - mention ai:Measure as parent concept and that we are looking to expand this in the future based on DPV TOMs taxonomies specifically for AI"></aside>
    <p>The concept [=Measure=] is a specific measure associated with AI technologies to address risks related to AI technologies. While the [[DPV]] and [[RISK]] extension provide relevance and modelling of measures along with detailed taxonomies, this concept is useful to represent the measures developed and specifically used for AI technologies. The DPVCG welcomes proposals and participation to further expand the taxonomy of measures.</p>
  </section>

</section>

<section id="vocab-lifecycle">
    <h2>Lifecycle</h2>
    <p>[=LifecycleStage=] models the lifecycle of AI technologies from its inception to deployment, use, and retirement. While we use the term 'lifecycle' here, these stages are also useful in other similar contexts such as 'AI Value Chain' and 'AI Supply Chain'. The AI-specific lifecycle is extended from the concept `tech:LifecycleStage` defined in the [[TECH]] extension to model lifecycle and stages of technologies in general. It can therefore be used with the existing relation `tech:hasLifecycleStage` to denote its applicability or involvement.</p>
    <aside class="note" title="Alignment with lifecycle of technology">
      <p>We are currently exploring the alignment of these concepts, which are based on ISO/IEC 22989:2022, with those for lifecycle of technology (in general) as defined in  ISO/IEC/IEEE 15288:2023 Systems and software engineering — System life cycle processes. We have proposed inclusion of technology lifecycle concepts in the [[TECH]] extension, which would then be extended here in the AI extension.</p>
    </aside>
    {{ list_hierarchy(modules['lifecycle']['classes']) }}
  </section>
  

  <section id="vocabulary">
<h2>Vocabulary Index</h2>
  <section id="dpv-classes">
    {{ index_concepts(vocab, vocab_name, filter="classes") }}
  </section>
  <section id="dpv-properties">
    {{ index_concepts(vocab, vocab_name, filter="properties") }}
  </section>
  <section id="external-concepts">
    <p>DPV uses the following terms from [[RDF]] and  [[RDFS]] with their defined meanings:</p>
    <ul>
      <li id="rdf:type"><dfn>rdf:type</dfn> to denote a concept is an instance of another concept</li>
      <li id="rdfs:Class"><dfn>rdfs:Class</dfn> to denote a concept is a Class or a category</li>
      <li id="rdfs:subClassOf"><dfn>rdfs:subClassOf</dfn> to specify the concept is a subclass (subtype, sub-category, subset) of another concept</li>
      <li id="rdf:Property"><dfn>rdf:Property</dfn> to denote a concept is a property or a relation</li>
      </ul>
    <p>The following external concepts are re-used within DPV:</p>
    {{ index_concepts(vocab, vocab_name, filter="external") }}
  </section>
</section>
  
{% if proposed %}
<section id="proposed-terms" class="appendix">
  <h2>Proposed Terms</h2>
  <p>The following terms have been proposed for inclusion, and are under discussion. They are provided here for illustrative purposes and should not be considered as part of DPV.</p>
    <ul>{% for term in proposed %}
      <li>{{term}}</li>
    {% endfor %}</ul>
</section>
{% endif %}

<section id="future-work" class="appendix">
  <h2>Future Work</h2>
  <aside class="issue" data-number="82"></aside>
  <aside class="issue" data-number="94"></aside>
  <aside class="issue" data-number="197"></aside>
</section>

{% block ACKNOWLEDGEMENTS %}
<section id="funding-acknowledgements" class="notoc">
  <h2>Funding Acknowledgements</h2>

  <h3>Funding Sponsors</h3>
  {{ funding_acknowledgements() }}

  <h3>Funding Acknowledgements for Contributors</h3>
  <p>The contributions of Delaram Golpayegani have received funding through the <a href="https://protect-network.eu/">PROTECT ITN Project</a> from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 813497, in particular through the development of <a href="https://w3id.org/airo">AI Risk Ontology (AIRO)</a> and <a href="https://w3id.org/vair">Vocabulary of AI Risks (VAIR)</a> which have been integrated in to this extension.</p>
  <p>The contributions of Harshvardhan J. Pandit and Delaram Golpayegani have been made with the financial support of Science Foundation Ireland under Grant Agreement No. 13/RC/2106_P2 at the ADAPT SFI Research Centre.</p>

</section>
{% endblock ACKNOWLEDGEMENTS %}

<section id="changelog">
  <h2>Changelog for v2.1</h2>
  <div class="issue" title="Changelog for v2.1"></div>
</section>

<script type="text/javascript" src="../diagrams/common.js" defer></script>
</body>
</html>
